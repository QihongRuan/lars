Testing the Waters: Behavior across Participant Pools
Journal: American Economic Review
Manuscript ID AER-2018-1065.R3
Manuscript Type: Administrator
Keywords: B41, C80, C90

Page 1 of 72

Disclosure Statement
Regarding the paper “Testing the Water: Behavior across Participant Pools,” I report:
No conflict of interest;
Financial support from the National Science Foundation;
No other significant sources of funds;
No position in relevant organizations.

Regards,

Erik Snowberg

Page 2 of 72

Disclosure Statement
Regarding the paper “Testing the Waters: Behavior across Participant Pools,” I report:
No conflict of interest;
Financial support from the National Science Foundation;
Financial support from the Betty and Gordon Moore Foundation;
No other significant sources of funds;
No position in relevant organizations.

Regards,

Leeat Yariv

Page 3 of 72

Testing the Waters:
Behavior across Participant Pools∗
Erik Snowberg
University of British
Columbia, CESifo, and NBER
snowberg@mail.ubc.ca
hss.caltech.edu/∼snowberg

Leeat Yariv
Princeton University,
CEPR, and NBER
lyariv@princeton.edu
www.princeton.edu/yariv

May 24, 2020

Abstract
We leverage a large-scale incentivized survey eliciting behaviors from (almost) an entire
undergraduate university student population, a representative sample of the U.S. population, and Amazon Mechanical Turk (MTurk) to address concerns about the external
validity of experiments with student participants. Behavior in the student population
offers bounds on behaviors in other populations, and correlations between behaviors
are similar across samples. Furthermore, non-student samples exhibit higher levels
of noise. Adding historical lab participation data, we find a small set of attributes
over which lab participants differ from non-lab participants. An additional set of lab
experiments shows no evidence of observer effects.

JEL Classifications: B41, C80, C90
Keywords: Lab Selection, External Validity, Experiments

∗

Snowberg gratefully acknowledges the support of NSF grants SES-1156154 and SMA-1329195. Yariv
gratefully acknowledges the support of NSF grant SES-1629613 and the Gordon and Betty Moore Foundation
grant 1158. We thank the Editor and four anonymous reviewers for many helpful suggestions. We also
thank Marina Agranov, Alessandra Casella, Armin Falk, Guillaume Fréchette, Drew Fudenberg, Johannes
Haushoffer, Salvatore Nunnari, Nichole Szembrot, Emanuel Vespa, and seminar audiences at Columbia
University, Cornell, NTU, UCSD, and University of Maryland for useful comments and encouragement.

Page 4 of 72

Lab experiments have been used to amass large amounts of data on human behavior when
facing economic incentives. Yet, skepticism persists about whether experimental insights can
be generalized beyond experimental labs and university-student populations. Critics express
concern that experiments are conducted on populations behaviorally distinct from those that
generally interest economists, and in environments unlike those in which people actually make
decisions. The paucity of data that could alleviate such concerns makes them difficult to
address directly.1
We provide a data-driven evaluation of several external validity concerns. Leveraging
unique data from the Caltech Cohort Study (CCS)—an incentivized, comprehensive behavioral survey of almost the entire undergraduate population of the California Institute
of Technology (Caltech)—we shed light on the behavioral differences between undergraduates and other populations, and assess whether behavior is different in the laboratory. In
particular, we provide evidence relevant to three questions. First, are university students
behaviorally different than representative populations or convenience samples, specifically
Amazon’s Mechanical Turk (MTurk)? Second, are those students who choose to participate
in lab experiments different from the general student population? Third, do students change
their behavior in the lab?
We show that elicited behaviors differ across our student, representative, and MTurk
samples. However, comparative statics and correlations are similar. Differences in correlations can largely be accounted for by statistical insignificance in representative and MTurk
samples, driven by higher levels of noise. We see some evidence of differences in observable
behaviors between the general student population and self-selected lab participants, though
these differences are confined to a minimal set of attributes that are easy to elicit and control
for. We see no evidence of participants behaving differently inside the lab than outside of it.
Our results suggest that experiments utilizing undergraduate students, in or outside the
lab, allow generalizable inferences about behavior. This is despite undergraduates differing
1

There is some useful work that examines these issues in specific settings. See the literature review for
details.

1

Page 5 of 72

in important ways from other populations. In addition, we document behavior patterns
that are useful both in choosing a venue and population for the execution of laboratory
experiments and for the interpretation of experimental results.
To address the questions listed above, we use a large-scale, online, incentivized survey
given to several different populations, as detailed in Section II.. The survey is designed
to elicit a battery of behavioral attributes: risk aversion, altruism, over-confidence, overprecision, implicit attitudes toward gender and race, various strategic interactions, and so
on. The main results of this paper rely on the implementation of this incentivized survey
using four different populations. First, in the CCS, described above, 90% of the entire
undergraduate population of Caltech participated. The second and third populations were a
representative sample of the U.S., and a convenience sample of U.S. residents from MTurk,
each containing approximately 1,000 participants. These datasets are unusually large for
experimental work, and assure that we can detect even relatively small differences. Finally,
we brought the CCS into the lab, where approximately 100 Caltech students completed the
same survey, but in a substantially different environment. In addition, we wed the CCS
data with historical data about lab participation in the Caltech Social Science Experimental
Laboratory (SSEL). We used two further samples to address specific questions: a sample
of another 1,264 MTurk participants to examine the effects of different incentive levels, and
a sample of 202 students from the University of British Columbia (UBC) laboratory to
demonstrate the possibility of running similar surveys using different student populations.
To answer each of our main questions we employ similar analytic methods. First, we
compare the mean levels of elicited behaviors in different samples. Then we compare the
underlying distributions. In general, statistically significant mean differences for a specific
elicitation are associated with a first-order stochastic dominance relation between the different samples. Differences in behavior may be crucial for policy. For example, they may
suggest when certain interventions are particularly desirable for specific groups. Nonetheless,
most studies in experimental economics inspect linkages between behaviors, attributes, and

2

